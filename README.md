# GPT_Implementation
Implementing a decoder-only self attention transformer
## Character-Level Transformer: Shakespeare Text Generation
This repository contains a character-level, self-attention decoder-only Transformer model implementation, trained on 1 million rows of Shakespeare's lines. The model is designed to generate text with a style inspired by the works of William Shakespeare.

### Features
1. Character-Level Language Model: Predicts the next character based on the input sequence.
2. Decoder-Only Transformer: Utilizes a self-attention mechanism for text generation, focusing solely on the decoder component.
3. Trained on Shakespeare Texts: Trained on a large dataset of Shakespeareâ€™s works to capture the unique language style and structure.
